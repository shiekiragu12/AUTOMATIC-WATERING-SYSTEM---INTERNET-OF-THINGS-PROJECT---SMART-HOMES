{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sqi5B7V_Rjim"
      },
      "outputs": [],
      "source": [
        "# Copyright 2025 Google LLC\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VyPmicX9RlZX"
      },
      "source": [
        "# Intro to Gemini 2.5 Pro\n",
        "\n",
        "\n",
        "<table align=\"left\">\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/generative-ai/blob/main/gemini/getting-started/intro_gemini_2_5_pro.ipynb\">\n",
        "      <img width=\"32px\" src=\"https://www.gstatic.com/pantheon/images/bigquery/welcome_page/colab-logo.svg\" alt=\"Google Colaboratory logo\"><br> Open in Colab\n",
        "    </a>\n",
        "  </td>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://console.cloud.google.com/vertex-ai/colab/import/https:%2F%2Fraw.githubusercontent.com%2FGoogleCloudPlatform%2Fgenerative-ai%2Fmain%2Fgemini%2Fgetting-started%2Fintro_gemini_2_5_pro.ipynb\">\n",
        "      <img width=\"32px\" src=\"https://lh3.googleusercontent.com/JmcxdQi-qOpctIvWKgPtrzZdJJK-J3sWE1RsfjZNwshCFgE_9fULcNpuXYTilIR2hjwN\" alt=\"Google Cloud Colab Enterprise logo\"><br> Open in Colab Enterprise\n",
        "    </a>\n",
        "  </td>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/GoogleCloudPlatform/generative-ai/main/gemini/getting-started/intro_gemini_2_5_pro.ipynb\">\n",
        "      <img src=\"https://www.gstatic.com/images/branding/gcpiconscolors/vertexai/v1/32px.svg\" alt=\"Vertex AI logo\"><br> Open in Vertex AI Workbench\n",
        "    </a>\n",
        "  </td>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/getting-started/intro_gemini_2_5_pro.ipynb\">\n",
        "      <img width=\"32px\" src=\"https://www.svgrepo.com/download/217753/github.svg\" alt=\"GitHub logo\"><br> View on GitHub\n",
        "    </a>\n",
        "  </td>\n",
        "</table>\n",
        "\n",
        "<div style=\"clear: both;\"></div>\n",
        "\n",
        "<b>Share to:</b>\n",
        "\n",
        "<a href=\"https://www.linkedin.com/sharing/share-offsite/?url=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/getting-started/intro_gemini_2_5_pro.ipynb\" target=\"_blank\">\n",
        "  <img width=\"20px\" src=\"https://upload.wikimedia.org/wikipedia/commons/8/81/LinkedIn_icon.svg\" alt=\"LinkedIn logo\">\n",
        "</a>\n",
        "\n",
        "<a href=\"https://bsky.app/intent/compose?text=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/getting-started/intro_gemini_2_5_pro.ipynb\" target=\"_blank\">\n",
        "  <img width=\"20px\" src=\"https://upload.wikimedia.org/wikipedia/commons/7/7a/Bluesky_Logo.svg\" alt=\"Bluesky logo\">\n",
        "</a>\n",
        "\n",
        "<a href=\"https://twitter.com/intent/tweet?url=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/getting-started/intro_gemini_2_5_pro.ipynb\" target=\"_blank\">\n",
        "  <img width=\"20px\" src=\"https://upload.wikimedia.org/wikipedia/commons/5/5a/X_icon_2.svg\" alt=\"X logo\">\n",
        "</a>\n",
        "\n",
        "<a href=\"https://reddit.com/submit?url=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/getting-started/intro_gemini_2_5_pro.ipynb\" target=\"_blank\">\n",
        "  <img width=\"20px\" src=\"https://redditinc.com/hubfs/Reddit%20Inc/Brand/Reddit_Logo.png\" alt=\"Reddit logo\">\n",
        "</a>\n",
        "\n",
        "<a href=\"https://www.facebook.com/sharer/sharer.php?u=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/getting-started/intro_gemini_2_5_pro.ipynb\" target=\"_blank\">\n",
        "  <img width=\"20px\" src=\"https://upload.wikimedia.org/wikipedia/commons/5/51/Facebook_f_logo_%282019%29.svg\" alt=\"Facebook logo\">\n",
        "</a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8MqT58L6Rm_q"
      },
      "source": [
        "| Authors |\n",
        "| --- |\n",
        "| [Eric Dong](https://github.com/gericdong) |\n",
        "| [Holt Skinner](https://github.com/holtskinner) |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nVxnv1D5RoZw"
      },
      "source": [
        "## Overview\n",
        "\n",
        "**YouTube Video: Introduction to Gemini on Vertex AI**\n",
        "\n",
        "<a href=\"https://www.youtube.com/watch?v=YfiLUpNejpE&list=PLIivdWyY5sqJio2yeg1dlfILOUO2FoFRx\" target=\"_blank\">\n",
        "  <img src=\"https://img.youtube.com/vi/YfiLUpNejpE/maxresdefault.jpg\" alt=\"Introduction to Gemini on Vertex AI\" width=\"500\">\n",
        "</a>\n",
        "\n",
        "[Gemini 2.5 Pro](https://cloud.google.com/vertex-ai/generative-ai/docs/models/gemini/2-5-pro) is Google's most advanced reasoning Gemini model, to solve complex problems. With the 2.5 series, the Gemini models are now hybrid reasoning models! Gemini 2.5 Pro can apply an extended amount of thinking across tasks, and use tools in order to maximize response accuracy.\n",
        "\n",
        "Gemini 2.5 Pro is:\n",
        "\n",
        "- A significant improvement from previous models across capabilities including coding, reasoning, and multimodality\n",
        "- Industry-leading in reasoning with state of the art performance in Math & STEM benchmarks\n",
        "- An amazing model for code, with particularly strong web development\n",
        "- Particularly good for complex prompts, while still being well rounded"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WfFPCBL4Hq8x"
      },
      "source": [
        "### Objectives\n",
        "\n",
        "In this tutorial, you will learn how to use the Gemini API and the Google Gen AI SDK for Python with the Gemini 2.5 Pro model.\n",
        "\n",
        "You will complete the following tasks:\n",
        "\n",
        "- Generate text\n",
        "- Control the thinking budget\n",
        "- View summarized thoughts\n",
        "- Configure model parameters\n",
        "- Set system instructions\n",
        "- Use safety filters\n",
        "- Start a multi-turn chat\n",
        "- Use controlled generation\n",
        "- Count tokens\n",
        "- Process multimodal (audio, code, documents, images, video) data\n",
        "- Use automatic and manual function calling\n",
        "- Code execution"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gPiTOAHURvTM"
      },
      "source": [
        "## Getting Started"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CHRZUpfWSEpp"
      },
      "source": [
        "### Install Google Gen AI SDK for Python\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "sG3_LKsWSD3A",
        "outputId": "aea26178-81c1-4c16-8dd4-cc898bb9e072",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/43.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.1/43.1 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/241.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━\u001b[0m \u001b[32m235.5/241.7 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m241.7/241.7 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "%pip install --upgrade --quiet google-genai"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HlMVjiAWSMNX"
      },
      "source": [
        "### Authenticate your notebook environment (Colab only)\n",
        "\n",
        "If you are running this notebook on Google Colab, run the cell below to authenticate your environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "12fnq4V0SNV3"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "\n",
        "if \"google.colab\" in sys.modules:\n",
        "    from google.colab import auth\n",
        "\n",
        "    auth.authenticate_user()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ve4YBlDqzyj9"
      },
      "source": [
        "### Set up Google Cloud Project or API Key for Vertex AI\n",
        "\n",
        "You'll need to set up authentication by choosing **one** of the following methods:\n",
        "\n",
        "1.  **Use a Google Cloud Project:** Recommended for most users, this requires enabling the Vertex AI API in your Google Cloud project.\n",
        "    - [Enable the Vertex AI API](https://console.cloud.google.com/flows/enableapi?apiid=aiplatform.googleapis.com)\n",
        "    - Run the cell below to set your project ID and location.\n",
        "    - Read more about [Supported locations](https://cloud.google.com/vertex-ai/generative-ai/docs/learn/locations)\n",
        "2.  **Use a Vertex AI API Key (Express Mode):** For quick experimentation.\n",
        "    - [Get an API Key](https://cloud.google.com/vertex-ai/generative-ai/docs/start/express-mode/overview)\n",
        "    - See tutorial [Getting started with Gemini using Vertex AI in Express Mode](https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/getting-started/intro_gemini_express.ipynb).\n",
        "\n",
        "This tutorial uses a Google Cloud Project for authentication."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "a3265ecb5f26"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "PROJECT_ID = \"friendlychat-c6dcb\"  # @param {type: \"string\", placeholder: \"[your-project-id]\", isTemplate: true}\n",
        "if not PROJECT_ID or PROJECT_ID == \"[your-project-id]\":\n",
        "    PROJECT_ID = str(os.environ.get(\"GOOGLE_CLOUD_PROJECT\"))\n",
        "\n",
        "LOCATION = \"global\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EdvJRUWRNGHE"
      },
      "source": [
        "### Import libraries\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "qgdSpVmDbdQ9"
      },
      "outputs": [],
      "source": [
        "from IPython.display import HTML, Image, Markdown, display\n",
        "from google import genai\n",
        "from google.genai.types import (\n",
        "    FunctionDeclaration,\n",
        "    GenerateContentConfig,\n",
        "    GoogleSearch,\n",
        "    HarmBlockThreshold,\n",
        "    HarmCategory,\n",
        "    Part,\n",
        "    SafetySetting,\n",
        "    ThinkingConfig,\n",
        "    Tool,\n",
        "    ToolCodeExecution,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "be18ac9c5ec8"
      },
      "source": [
        "### Create a client"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "3870ef96f984"
      },
      "outputs": [],
      "source": [
        "client = genai.Client(vertexai=True, project=PROJECT_ID, location=LOCATION)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n4yRkFg6BBu4"
      },
      "source": [
        "## Use the Gemini 2.5 Pro model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eXHJi5B6P5vd"
      },
      "source": [
        "### Load the Gemini 2.5 Pro model\n",
        "\n",
        "Learn more about all [Gemini models on Vertex AI](https://cloud.google.com/vertex-ai/generative-ai/docs/learn/models#gemini-models)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "-coEslfWPrxo"
      },
      "outputs": [],
      "source": [
        "MODEL_ID = \"gemini-2.5-pro\"  # @param {type: \"string\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "37CH91ddY9kG"
      },
      "source": [
        "### Generate text from text prompts\n",
        "\n",
        "Use the `generate_content()` method to generate responses to your prompts.\n",
        "\n",
        "You can pass text to `generate_content()`, and use the `.text` property to get the text content of the response.\n",
        "\n",
        "By default, Gemini outputs formatted text using [Markdown](https://daringfireball.net/projects/markdown/) syntax."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "xRJuHj0KZ8xz",
        "outputId": "11b3e0a8-7385-453d-fe3d-525583c3b6a0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 151
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "That's an easy one! The largest planet in our solar system is **Jupiter**.\n\nIt's a true giant, so massive that it's more than twice as massive as all the other planets in our solar system combined.\n\nTo give you an idea of its scale:\n*   **Volume:** You could fit about **1,300 Earths** inside of it.\n*   **Diameter:** Its diameter is about 11 times that of Earth.\n*   **Type:** It's a gas giant, composed mainly of hydrogen and helium, similar to a star."
          },
          "metadata": {}
        }
      ],
      "source": [
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID, contents=\"What's the largest planet in our solar system?\"\n",
        ")\n",
        "\n",
        "display(Markdown(response.text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JkYQATRxAK1_"
      },
      "source": [
        "#### Example prompts\n",
        "\n",
        "- What are the biggest challenges facing the healthcare industry?\n",
        "- What are the latest developments in the automotive industry?\n",
        "- What are the biggest opportunities in retail industry?\n",
        "- (Try your own prompts!)\n",
        "\n",
        "For more examples of prompt engineering, refer to [this notebook](https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/prompts/intro_prompt_design.ipynb)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "95aeab702af3"
      },
      "source": [
        "### Control the thinking budget\n",
        "\n",
        "You set the optional `thinking_budget` parameter in the `ThinkingConfig` to control and configure how much a model thinks on a given user prompt. The `thinking_budget` sets the upper limit on the number of tokens to use for reasoning for certain tasks. It allows users to control quality and speed of response.\n",
        "\n",
        "**Notes**\n",
        "\n",
        "- By default, the model automatically controls how much it thinks up to a maximum of 8192 tokens.\n",
        "- The maximum thinking budget that you can set is `32768` tokens, and the minimum you can set is `128`.\n",
        "\n",
        "Then use the `generate_content` or `generate_content_stream` method to send a request to generate content with the `thinking_config`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "364133e30ba5",
        "outputId": "21b10142-f0d5-461b-f672-5e937d014752",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "There are three R's in the word st**r**awbe**rr**y."
          },
          "metadata": {}
        }
      ],
      "source": [
        "THINKING_BUDGET = 1024  # @param {type: \"integer\"}\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=\"How many R's are in the word strawberry?\",\n",
        "    config=GenerateContentConfig(\n",
        "        thinking_config=ThinkingConfig(\n",
        "            thinking_budget=THINKING_BUDGET,\n",
        "        )\n",
        "    ),\n",
        ")\n",
        "\n",
        "display(Markdown(response.text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "05dc39e0c6b5"
      },
      "source": [
        "Optionally, you can print the usage_metadata and token counts from the model response."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "7981c3442177",
        "outputId": "1cbc48dc-7b27-4343-e495-d774b6920e51",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt_token_count: 11\n",
            "candidates_token_count: 20\n",
            "thoughts_token_count: 250\n",
            "total_token_count: 281\n"
          ]
        }
      ],
      "source": [
        "print(f\"prompt_token_count: {response.usage_metadata.prompt_token_count}\")\n",
        "print(f\"candidates_token_count: {response.usage_metadata.candidates_token_count}\")\n",
        "print(f\"thoughts_token_count: {response.usage_metadata.thoughts_token_count}\")\n",
        "print(f\"total_token_count: {response.usage_metadata.total_token_count}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c66712160c15"
      },
      "source": [
        "### View summarized thoughts\n",
        "\n",
        "You can optionally set the `include_thoughts` flag to enable the model to generate and return a summary of the \"thoughts\" that it generates in addition to the final answer.\n",
        "\n",
        "In this example, you use the `generate_content` method to send a request to generate content with summarized thoughts. The model responds with multiple parts, the thoughts and the model response. You can check the `part.thought` field to determine if a part is a thought or not."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "60d74a351671",
        "outputId": "f34449e7-8fce-4a99-ced9-b5a3e877ee55",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "## Thoughts:\n         Alright, let's break this down. First, the user's question is clear: they want to know how many \"R\"s are in the word \"strawberry.\" Easy enough. So, the word in question is \"strawberry.\" Now, I need to scan it meticulously, looking for each instance of the letter \"R.\" Let's see... \"s\" - no; \"t\" - nope; ah, there's an \"r\"! Mark one. Then \"a\", \"w\", \"b\", \"e\"... and another \"r\"! Two so far. And finally, *another* \"r\" at the end. Okay, so three \"R\"s in total. Now to formulate the answer clearly. The most direct approach is best here. I'll start with a simple, direct answer: \"There are three R's in the word strawberry.\" For added clarity, and to visually confirm my count, I can throw in a quick breakdown: \"s-t-r-a-w-b-e-r-r-y.\" That should be absolutely unambiguous.\n\n        "
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "## Answer:\n         There are **three** R's in the word \"strawberry\".\n        "
          },
          "metadata": {}
        }
      ],
      "source": [
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=\"How many R's are in the word strawberry?\",\n",
        "    config=GenerateContentConfig(\n",
        "        thinking_config=ThinkingConfig(\n",
        "            include_thoughts=True,\n",
        "        )\n",
        "    ),\n",
        ")\n",
        "\n",
        "for part in response.candidates[0].content.parts:\n",
        "    if part.thought:\n",
        "        display(\n",
        "            Markdown(\n",
        "                f\"\"\"## Thoughts:\n",
        "         {part.text}\n",
        "        \"\"\"\n",
        "            )\n",
        "        )\n",
        "    else:\n",
        "        display(\n",
        "            Markdown(\n",
        "                f\"\"\"## Answer:\n",
        "         {part.text}\n",
        "        \"\"\"\n",
        "            )\n",
        "        )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6lLIxqS6_-l8"
      },
      "source": [
        "### Generate content stream\n",
        "\n",
        "By default, the model returns a response after completing the entire generation process. You can also use the `generate_content_stream` method to stream the response as it is being generated, and the model will return chunks of the response as soon as they are generated.\n",
        "\n",
        "This example shows how to set the `include_thoughts` and `thinking_budget` in the `generate_content_stream` method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "ZiwWBhXsAMnv",
        "outputId": "c8e409d3-0985-4453-c0fc-dcefd46b756d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 537
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "## Thoughts"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**Calculating the Cost**\n\nI'm focusing on the mathematical relationships now. I've deconstructed the core information provided in the riddle. The equation \"Bat = Ball + $1.00\" seems key. Now, the challenge is to calculate the individual costs of the bat and the ball to answer the question. I'm considering substitution to solve this.\n\n\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**Unpacking the Problem**\n\nI've identified the underlying cognitive bias at play and the common System 1 trap. I've rejected the intuitive answer of $0.10 as inconsistent. Now, I'm focusing on the logical, algebraic approach. I'm going to define variables, setting 'B' as the ball's cost and using the provided equations to derive the correct answer, which I expect to be a lower cost than my incorrect intuitive response.\n\n\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**Confirming with Algebra**\n\nI've rigorously solved the problem. Using algebraic substitution, I've confirmed the ball costs $0.05. The bat, therefore, costs $1.05, adhering to all provided information. The exercise demonstrates the importance of careful, step-by-step logical reasoning to avoid cognitive pitfalls, which I've successfully navigated.\n\n\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**Decomposing the Equations**\n\nI'm refining my approach further. I'm translating the provided problem statements into algebraic equations, establishing variables to represent each unknown. I'm particularly focusing on the relationships to isolate and then determine the cost of the ball. I'm anticipating the correct value, knowing the riddle's nature.\n\n\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "## Answer"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "This is a classic brain teaser! The intuitive answer is 10 cents, but that's not quite right.\n\nHere's the breakdown:\n\nThe ball costs **5 cents**.\n\nHere's why:\n*   The ball costs $0.05\n*   The bat costs $1."
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "00 more than the ball, so it costs $1.05.\n*   The total cost is $0.05 (ball) + $1.05 (bat) = **$1.10**."
          },
          "metadata": {}
        }
      ],
      "source": [
        "THINKING_BUDGET = 1024  # @param {type: \"integer\"}\n",
        "INCLUDE_THOUGHTS = True  # @param {type: \"boolean\"}\n",
        "\n",
        "prompt = \"\"\"\n",
        "A bat and a ball cost $1.10 in total.\n",
        "The bat costs $1.00 more than the ball.\n",
        "How much does the ball cost?\n",
        "\"\"\"\n",
        "\n",
        "thoughts = \"\"\n",
        "answer = \"\"\n",
        "\n",
        "for chunk in client.models.generate_content_stream(\n",
        "    model=MODEL_ID,\n",
        "    contents=prompt,\n",
        "    config=GenerateContentConfig(\n",
        "        thinking_config=ThinkingConfig(\n",
        "            thinking_budget=THINKING_BUDGET,\n",
        "            include_thoughts=INCLUDE_THOUGHTS,\n",
        "        )\n",
        "    ),\n",
        "):\n",
        "\n",
        "    for part in chunk.candidates[0].content.parts:\n",
        "        if not part.text:\n",
        "            continue\n",
        "        elif part.thought:\n",
        "            if not thoughts:\n",
        "                display(Markdown(\"## Thoughts\"))\n",
        "            display(Markdown(part.text))\n",
        "            thoughts += part.text\n",
        "        else:\n",
        "            if not answer:\n",
        "                display(Markdown(\"## Answer\"))\n",
        "            display(Markdown(part.text))\n",
        "            answer += part.text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "df5a184feb04"
      },
      "source": [
        "## Thinking examples\n",
        "\n",
        "The following examples are some complex tasks that require multiple rounds of strategizing and iteratively solving.\n",
        "\n",
        "### **Thinking example 1**: Code generation\n",
        "\n",
        "Gemini 2.5 Pro excels at creating visually compelling web apps and agentic code applications, along with code transformation and editing.\n",
        "\n",
        "Let's see how the model uses its reasoning capabilities to create a video game, using executable code from a single line prompt. See the example game [here](https://www.youtube.com/watch?v=RLCBSpgos6s)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "598bafe38bba",
        "outputId": "ce81a111-07f5-488a-e7f7-28d8520aa5ae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Of course! Here is a complete, self-contained p5.js endless runner game featuring a pixelated dinosaur, interesting parallax backgrounds, and on-screen instructions.\n\nJust copy and paste this entire code block into the [p5.js Web Editor](https://editor.p5js.org/) and press play.\n\n### Features:\n*   **Pixel Art:** All graphics, including the dinosaur and cacti, are drawn procedurally for a crisp, pixelated look.\n*   **Parallax Scrolling:** A multi-layered background with mountains, hills, and clouds moves at different speeds to create a sense of depth.\n*   **Smooth Animation:** The dino has a simple but effective running animation.\n*   **Progressive Difficulty:** The game speeds up over time, making it more challenging.\n*   **On-Screen UI:** All instructions, the score, and game over messages are displayed directly on the canvas.\n*   **Responsive Design:** The canvas will adapt to the size of your browser window.\n\n```javascript\n// --- DINO RUNNER ---\n// A captivating endless runner by AI\n// INSTRUCTIONS:\n// - Press [SPACE] to JUMP\n// - Avoid the cacti!\n// - Press [ENTER] to restart after a Game Over\n\nlet dino;\nlet obstacles = [];\nlet score = 0;\nlet highScore = 0;\nlet gameSpeed = 6;\nlet initialGameSpeed = 6;\nlet gameSpeedAcceleration = 0.001;\n\nlet gameState = 'START'; // START, PLAYING, GAME_OVER\n\n// --- Background Layers ---\nlet bgLayers = [];\nconst groundLevelOffset = 100; // How high the ground is from the bottom\n\n// --- Game Colors ---\nconst COLOR_SKY = [135, 206, 235];\nconst COLOR_GROUND = [222, 184, 135];\nconst COLOR_DINO = [80, 140, 80];\nconst COLOR_OBSTACLE = [0, 100, 0];\nconst COLOR_TEXT = [80, 80, 80];\nconst COLOR_SUN = [255, 223, 0];\nconst COLOR_MOUNTAIN_FAR = [170, 190, 200];\nconst COLOR_MOUNTAIN_NEAR = [140, 160, 170];\nconst COLOR_CLOUD = [255, 255, 255, 200];\n\n\nfunction setup() {\n  createCanvas(windowWidth, windowHeight);\n  noSmooth(); // Essential for the pixelated look\n  textAlign(CENTER, CENTER);\n\n  // Initialize background layers\n  // Each layer has a speed multiplier and a function to generate its elements\n  bgLayers.push(createLayer(0.1, 3, createMountain, COLOR_MOUNTAIN_FAR, 300, 500));\n  bgLayers.push(createLayer(0.3, 5, createMountain, COLOR_MOUNTAIN_NEAR, 200, 400));\n  bgLayers.push(createLayer(0.5, 10, createCloud, COLOR_CLOUD, 100, 200));\n\n  resetGame();\n}\n\nfunction draw() {\n  // State machine for game flow\n  switch (gameState) {\n    case 'START':\n      drawStartScreen();\n      break;\n    case 'PLAYING':\n      drawGame();\n      break;\n    case 'GAME_OVER':\n      drawGameOverScreen();\n      break;\n  }\n}\n\n// --- GAME STATE FUNCTIONS ---\n\nfunction drawStartScreen() {\n  background(COLOR_SKY);\n  drawBackground();\n  drawGround();\n  \n  // Draw a stationary dino\n  dino.y = height - groundLevelOffset;\n  dino.show(false); // isRunning = false\n\n  // Draw Title and Instructions\n  fill(COLOR_TEXT);\n  textSize(64);\n  textFont('monospace');\n  text('PIXEL DINO RUN', width / 2, height / 3);\n  textSize(24);\n  text('Press [SPACE] or Click to Start', width / 2, height / 2);\n}\n\nfunction drawGame() {\n  background(COLOR_SKY);\n  drawBackground();\n  drawGround();\n\n  // Update and display score\n  score++;\n  fill(COLOR_TEXT);\n  textSize(24);\n  textFont('monospace');\n  textAlign(RIGHT);\n  text(`HI ${highScore.toString().padStart(5, '0')}  ${score.toString().padStart(5, '0')}`, width - 20, 30);\n  textAlign(CENTER);\n\n  // Manage Obstacles\n  if (frameCount % 90 === 0) {\n    if (random(1) > 0.3) {\n      obstacles.push(new Obstacle());\n    }\n  }\n\n  for (let i = obstacles.length - 1; i >= 0; i--) {\n    obstacles[i].update();\n    obstacles[i].show();\n\n    if (dino.hits(obstacles[i])) {\n      gameOver();\n    }\n\n    if (obstacles[i].isOffscreen()) {\n      obstacles.splice(i, 1);\n    }\n  }\n\n  // Update and display Dino\n  dino.update();\n  dino.show(true); // isRunning = true\n\n  // Increase difficulty\n  gameSpeed += gameSpeedAcceleration;\n}\n\nfunction drawGameOverScreen() {\n  background(COLOR_SKY);\n  drawBackground(false); // Stop background animation\n  drawGround();\n  \n  // Draw all objects as they were at the moment of game over\n  for (let o of obstacles) {\n    o.show();\n  }\n  dino.show(false);\n\n  // Display Game Over text\n  fill(200, 0, 0, 200);\n  rect(0, 0, width, height);\n  \n  fill(255);\n  textSize(64);\n  textFont('monospace');\n  text('GAME OVER', width / 2, height / 3);\n  \n  textSize(32);\n  text(`Score: ${score}`, width / 2, height / 2);\n  text(`High Score: ${highScore}`, width / 2, height / 2 + 50);\n\n  textSize(24);\n  text('Press [ENTER] to try again', width / 2, height * 0.7);\n}\n\n\n// --- GAME LOGIC & UTILITIES ---\n\nfunction resetGame() {\n  score = 0;\n  gameSpeed = initialGameSpeed;\n  obstacles = [];\n  dino = new Dino();\n  // Spawn the first obstacle further away to give the player time\n  obstacles.push(new Obstacle(width * 1.5)); \n  gameState = 'PLAYING';\n}\n\nfunction gameOver() {\n  if (score > highScore) {\n    highScore = score;\n  }\n  gameState = 'GAME_OVER';\n}\n\nfunction keyPressed() {\n  if (gameState === 'PLAYING' && (key === ' ' || keyCode === UP_ARROW)) {\n    dino.jump();\n  } else if (gameState === 'GAME_OVER' && keyCode === ENTER) {\n    resetGame();\n  } else if (gameState === 'START' && (key === ' ' || keyCode === UP_ARROW)) {\n    resetGame();\n  }\n}\n\nfunction mousePressed() {\n    if (gameState === 'START') {\n        resetGame();\n    } else if (gameState === 'PLAYING') {\n        dino.jump();\n    }\n}\n\nfunction windowResized() {\n  resizeCanvas(windowWidth, windowHeight);\n  // Re-initialize background layers on resize to fit the new screen width\n  bgLayers = [];\n  bgLayers.push(createLayer(0.1, 3, createMountain, COLOR_MOUNTAIN_FAR, 300, 500));\n  bgLayers.push(createLayer(0.3, 5, createMountain, COLOR_MOUNTAIN_NEAR, 200, 400));\n  bgLayers.push(createLayer(0.5, 10, createCloud, COLOR_CLOUD, 100, 200));\n}\n\n// --- DRAWING FUNCTIONS ---\n\nfunction drawGround() {\n  fill(COLOR_GROUND);\n  noStroke();\n  rect(0, height - groundLevelOffset, width, groundLevelOffset);\n}\n\nfunction drawBackground(animate = true) {\n  // Sun\n  fill(COLOR_SUN);\n  noStroke();\n  ellipse(width * 0.8, height * 0.2, 100, 100);\n\n  // Draw each layer\n  for (const layer of bgLayers) {\n    fill(layer.color);\n    noStroke();\n    for (const element of layer.elements) {\n      layer.drawFunc(element.x, element.y, element.w, element.h);\n      if (animate) {\n        element.x -= gameSpeed * layer.speed;\n        if (element.x + element.w < 0) {\n          element.x = width + random(50, 200);\n        }\n      }\n    }\n  }\n}\n\n// --- BACKGROUND GENERATION ---\n\nfunction createLayer(speed, numElements, drawFunc, color, minH, maxH) {\n  let layer = {\n    speed: speed,\n    elements: [],\n    drawFunc: drawFunc,\n    color: color\n  };\n  for (let i = 0; i < numElements; i++) {\n    layer.elements.push({\n      x: random(width),\n      y: height - groundLevelOffset - random(minH * 0.5, minH),\n      w: random(100, 300),\n      h: random(minH, maxH)\n    });\n  }\n  return layer;\n}\n\nfunction createMountain(x, y, w, h) {\n  triangle(x, y + h, x + w / 2, y, x + w, y + h);\n}\n\nfunction createCloud(x, y, w, h) {\n  ellipse(x, y, w, h);\n  ellipse(x + w * 0.4, y - h * 0.2, w * 0.8, h * 0.8);\n  ellipse(x - w * 0.4, y + h * 0.1, w * 0.7, h * 0.7);\n}\n\n\n// --- CLASSES ---\n\nclass Dino {\n  constructor() {\n    this.w = 40;\n    this.h = 60;\n    this.x = 80;\n    this.y = height - groundLevelOffset;\n    this.yVelocity = 0;\n    this.gravity = 0.8;\n    this.lift = -20;\n    this.groundY = height - groundLevelOffset;\n  }\n\n  jump() {\n    // Can only jump if on the ground\n    if (this.y >= this.groundY) {\n      this.yVelocity = this.lift;\n    }\n  }\n\n  hits(obstacle) {\n    // Simple Axis-Aligned Bounding Box collision detection\n    // The hitboxes are slightly smaller than the visual representation\n    let dinoHB = { x: this.x + 5, y: this.y - this.h, w: this.w - 10, h: this.h - 5 };\n    let obsHB = { x: obstacle.x, y: obstacle.y - obstacle.h, w: obstacle.w, h: obstacle.h };\n\n    return (\n      dinoHB.x < obsHB.x + obsHB.w &&\n      dinoHB.x + dinoHB.w > obsHB.x &&\n      dinoHB.y < obsHB.y + obsHB.h &&\n      dinoHB.y + dinoHB.h > obsHB.y\n    );\n  }\n\n  update() {\n    this.yVelocity += this.gravity;\n    this.y += this.yVelocity;\n\n    // Prevent dino from falling through the ground\n    if (this.y > this.groundY) {\n      this.y = this.groundY;\n      this.yVelocity = 0;\n    }\n  }\n\n  show(isRunning) {\n    push();\n    translate(this.x, this.y - this.h);\n    noStroke();\n    fill(COLOR_DINO);\n    \n    // Body\n    rect(10, 0, 25, 40);\n    // Head\n    rect(25, 0, 15, 20);\n    rect(35, 15, 10, 5); // Snout\n    // Tail\n    rect(0, 10, 10, 10);\n    // Arm\n    rect(25, 25, 5, 10);\n    \n    // Legs (Animate if running)\n    if (isRunning && frameCount % 20 < 10) {\n      // Leg 1\n      rect(12, 40, 8, 20);\n      // Leg 2\n      rect(25, 40, 8, 15);\n    } else {\n      // Leg 1\n      rect(12, 40, 8, 15);\n      // Leg 2\n      rect(25, 40, 8, 20);\n    }\n    \n    pop();\n  }\n}\n\nclass Obstacle {\n  constructor(startX) {\n    this.x = startX || width;\n    this.w = 20 + random(-5, 20); // Width of cactus\n    this.h = 40 + random(-10, 40); // Height of cactus\n    this.y = height - groundLevelOffset;\n    \n    // Randomly generate cactus \"arms\"\n    this.arms = [];\n    if (this.h > 40 && random(1) > 0.4) {\n        this.arms.push({\n            x: random(0.2, 0.8) * this.w,\n            y: -this.h * random(0.3, 0.7),\n            w: 10,\n            h: 20\n        });\n    }\n  }\n\n  update() {\n    this.x -= gameSpeed;\n  }\n\n  isOffscreen() {\n    return this.x + this.w < 0;\n  }\n\n  show() {\n    push();\n    translate(this.x, this.y - this.h);\n    noStroke();\n    fill(COLOR_OBSTACLE);\n    \n    // Main stalk\n    rect(0, 0, this.w, this.h);\n    \n    // Arms\n    for(let arm of this.arms) {\n        rect(arm.x, arm.y, arm.w, arm.h);\n    }\n    \n    pop();\n  }\n}\n```"
          },
          "metadata": {}
        }
      ],
      "source": [
        "prompt = \"\"\"\n",
        "  Make me a captivating endless runner game. Key instructions on the screen. p5js scene, no HTML.\n",
        "  I like pixelated dinosaurs and interesting backgrounds.\n",
        "\"\"\"\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=prompt,\n",
        "    config=GenerateContentConfig(\n",
        "        thinking_config=ThinkingConfig(\n",
        "            thinking_budget=8196,\n",
        "        )\n",
        "    ),\n",
        ")\n",
        "\n",
        "display(Markdown(response.text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9f00955d5b33"
      },
      "source": [
        "### **Thinking example 2**: Multimodal reasoning (Geometry)\n",
        "\n",
        "This geometry problem requires complex reasoning and is also using multimodal capabilities to reason across text and image."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "1b9975dcd0f0",
        "outputId": "2326033c-c8f2-4db6-b940-d2e36b44f96d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 409
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<img src=\"https://storage.googleapis.com/generativeai-downloads/images/geometry.png\" width=\"400\"/>"
            ],
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "image_file_url = (\n",
        "    \"https://storage.googleapis.com/generativeai-downloads/images/geometry.png\"\n",
        ")\n",
        "display(Image(url=image_file_url, width=400))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "43843bf748f1",
        "outputId": "e328be6b-48f8-463a-b663-f22a91093453",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 649
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "## Thoughts:\n         Alright, let's get to work. This geometry problem is about finding the area where a circle and a triangle overlap. First things first, I need to understand what's being presented. Okay, I see a light blue circle intersecting a light green right-angled triangle. The \"3\" labels, those are *crucial*.\n\nI see three lines coming from the center of the circle, two forming the right angle of the triangle. Those \"3\"s, going from the center to the circle's edge, *must* be radii. So, the radius (r) is 3. The lines originating from the center to the edges of the triangle are labeled \"3\" - they are radii.\n\nThe legs of the triangle, meeting at a right angle, also have \"3\" labels on them. The right angle sits *at the center* of the circle. Boom! That means the overlapping region is a circular sector, with a 90-degree angle. The angle is *defined* by the right angle.\n\nNow, area calculation. The sector's area is a fraction of the full circle's area. Area_sector = (θ / 360°) * π * r². We have r=3 and θ = 90 degrees. That's one-quarter of the circle's area. The full circle is π * 3² = 9π. One-quarter of that is 9π/4.\n\nI could use radians too, just to double-check. Same result: (1/2) * r² * θ_rad = (1/2) * 3² * (π/2) = 9π/4.\n\nGot it. The overlapping area is a sector, precisely a quarter of the circle.\n\n        "
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "## Answer:\n         Based on the image, we can determine the area of the overlapping region by following these steps:\n\n1.  **Identify the shapes:** We have a circle and a right-angled triangle.\n2.  **Analyze the overlap:** The vertex of the triangle's right angle is located at the center of the circle. The two legs of the triangle lie along two radii of the circle. This means the overlapping region is a sector of the circle.\n3.  **Find the radius of the circle (r):** The image labels the lines from the center to the edge of the circle as \"3\". This means the radius of the circle is 3.\n4.  **Find the angle of the sector (θ):** The sector is formed by the right angle of the triangle. A right angle is 90°.\n5.  **Calculate the area:** The area of a circular sector is a fraction of the total area of the circle. The formula is:\n\n    Area_sector = (θ / 360°) * π * r²\n\n    Plugging in our values:\n    *   θ = 90°\n    *   r = 3\n\n    Area = (90° / 360°) * π * (3)²\n    Area = (1/4) * π * 9\n    Area = 9π / 4\n\nThe area of the overlapping region is **9π/4**.\n\nAs a decimal, this is approximately **7.07**.\n        "
          },
          "metadata": {}
        }
      ],
      "source": [
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=[\n",
        "        Part.from_uri(file_uri=image_file_url, mime_type=\"image/png\"),\n",
        "        \"What's the area of the overlapping region?\",\n",
        "    ],\n",
        "    config=GenerateContentConfig(\n",
        "        thinking_config=ThinkingConfig(\n",
        "            include_thoughts=True,\n",
        "        )\n",
        "    ),\n",
        ")\n",
        "\n",
        "for part in response.candidates[0].content.parts:\n",
        "    if part.thought:\n",
        "        display(\n",
        "            Markdown(\n",
        "                f\"\"\"## Thoughts:\n",
        "         {part.text}\n",
        "        \"\"\"\n",
        "            )\n",
        "        )\n",
        "    else:\n",
        "        display(\n",
        "            Markdown(\n",
        "                f\"\"\"## Answer:\n",
        "         {part.text}\n",
        "        \"\"\"\n",
        "            )\n",
        "        )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ddf356ac9cce"
      },
      "source": [
        "### **Thinking example 3**:  Math and problem solving\n",
        "\n",
        "Here's another brain teaser based on an image, this time it looks like a mathematical problem, but it cannot actually be solved mathematically. If you check the thoughts of the model you'll see that it will realize it and come up with an out-of-the-box solution."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "447f05072790",
        "outputId": "24ea2547-55e6-49d3-83b7-d1ddc4c90faf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 348
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<img src=\"https://storage.googleapis.com/generativeai-downloads/images/pool.png\" width=\"400\"/>"
            ],
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "image_file_url = \"https://storage.googleapis.com/generativeai-downloads/images/pool.png\"\n",
        "display(Image(url=image_file_url, width=400))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "dc1faf95ce6f",
        "outputId": "8e574f2a-3ac8-40a2-d534-b48fef13bec3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 399
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "## Thoughts:\n         Alright, let's see. The user wants to add up three pool balls to get 30. Seems simple enough, but... hmm. First, I'll take a look at those numbers: 7, 9, 11, and 13. No combination of three of those adds up to 30. That's not going to work, I knew that right away. Okay, so it's not a straightforward calculation. Must be a riddle or a trick.\n\nI need to think outside the box. What am I missing? It's not just about the numbers; there's got to be a clever twist. Let's look at that image again, focusing on each detail: The pool balls. They're arranged on a felt table. What if the way the numbers are presented are also a hint?\n\nAha! The 9... could it be turned upside down to make a 6? Let's explore that. If we consider 6 instead of 9, our set becomes 7, 6, 11, and 13. Now, let's try our sums again. Bingo! 6 plus 11 plus 13 equals 30. Classic! This is a good riddle.\n\n        "
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "## Answer:\n         This is a classic riddle!\n\nAt first glance, it seems impossible because none of the combinations of three balls add up to 30:\n*   7 + 9 + 11 = 27\n*   7 + 9 + 13 = 29\n*   7 + 11 + 13 = 31\n*   9 + 11 + 13 = 33\n\nThe trick is to physically turn one of the balls upside down. If you turn the **9** ball upside down, it becomes a **6**.\n\nThen, you can use these three balls:\n**6 + 11 + 13 = 30**\n        "
          },
          "metadata": {}
        }
      ],
      "source": [
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=[\n",
        "        Part.from_uri(file_uri=image_file_url, mime_type=\"image/png\"),\n",
        "        \"How do I use three of the pool balls to sum up to 30?\",\n",
        "    ],\n",
        "    config=GenerateContentConfig(\n",
        "        thinking_config=ThinkingConfig(\n",
        "            include_thoughts=True,\n",
        "        )\n",
        "    ),\n",
        ")\n",
        "\n",
        "for part in response.candidates[0].content.parts:\n",
        "    if part.thought:\n",
        "        display(\n",
        "            Markdown(\n",
        "                f\"\"\"## Thoughts:\n",
        "         {part.text}\n",
        "        \"\"\"\n",
        "            )\n",
        "        )\n",
        "    else:\n",
        "        display(\n",
        "            Markdown(\n",
        "                f\"\"\"## Answer:\n",
        "         {part.text}\n",
        "        \"\"\"\n",
        "            )\n",
        "        )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cbc646bff621"
      },
      "source": [
        "For the remaining examples, we will set thinking budget to `128` to reduce latency, as they don't need extra reasoning capabilities."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "9254cfd0441b"
      },
      "outputs": [],
      "source": [
        "thinking_config = ThinkingConfig(thinking_budget=128)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hIJVEr0RQY8S"
      },
      "source": [
        "## Configure model parameters\n",
        "\n",
        "You can include parameter values in each call that you send to a model to control how the model generates a response. The model can generate different results for different parameter values. You can experiment with different model parameters to see how the results change.\n",
        "\n",
        "- Learn more about [experimenting with parameter values](https://cloud.google.com/vertex-ai/generative-ai/docs/learn/prompts/adjust-parameter-values).\n",
        "\n",
        "- See a list of all [Gemini API parameters](https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/inference#parameters).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "d9NXP5N2Pmfo",
        "outputId": "43f2790b-29cc-4bcb-e227-5c9d6257c243",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 361
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "*Woof!* *Wags tail.* Oh, a good puppy! Yes, you are! Look what I have! *Squeak squeak!*\n\nOkay, you want to know about the Big Thing We Stare At, the internet? It's like a giant, magical field full of squeaky toys!\n\n*   **You have a special squeaky toy.** It's called your **Computer** (or the small glowing rectangle the Human holds). When you want to play, you give your squeaky toy a little bite. *Squeak!* That's you wanting something, like a picture of a tasty treat.\n\n*   This *Squeak!* isn't a normal squeak. It's a special, magic squeak that zooms out of your house! It travels on invisible bouncy leashes called **Wi-Fi** or in tiny tunnels under the ground.\n\n*   The magic squeak zips and zaps until it finds a HUGE toy box. We call this toy box a **Server**. It’s not just any toy box; it's a super-organized one where a very smart Golden Retriever has put away millions and millions of squeaky toys. There are picture toys, movie toys, sound toys... all the toys!\n\n*   The Golden Retriever in the toy box hears your magic *Squeak!* and says, \"Aha! This good puppy wants the squeaky toy that looks like a tasty treat! I know exactly where that is!\"\n\n*   He grabs the correct squeaky toy (the picture of the treat) and makes a copy of its squeak. He then sends that squeak zooming right back to you on the same invisible leashes and tunnels. *Zoooooom!*\n\n*   Your special squeaky toy (your computer) catches the squeak-copy! It turns that magic squeak into the picture you wanted to see! And there it is on the screen! The tasty treat!\n\nSo, the internet is just you, sending a magic squeak to a giant toy box far away, and a nice doggo finding the toy you asked for and squeaking it right back at you!\n\nGood puppy! You're a smart puppy! Now, who wants to chase a REAL squeaky toy? *Squeak! Squeak! Squeak!*"
          },
          "metadata": {}
        }
      ],
      "source": [
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=\"Tell me how the internet works, but pretend I'm a puppy who only understands squeaky toys.\",\n",
        "    config=GenerateContentConfig(\n",
        "        temperature=2.0,\n",
        "        top_p=0.95,\n",
        "        candidate_count=1,\n",
        "        max_output_tokens=8000,\n",
        "        thinking_config=thinking_config,\n",
        "    ),\n",
        ")\n",
        "\n",
        "display(Markdown(response.text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "El1lx8P9ElDq"
      },
      "source": [
        "## Set system instructions\n",
        "\n",
        "[System instructions](https://cloud.google.com/vertex-ai/generative-ai/docs/learn/prompts/system-instruction-introduction) allow you to steer the behavior of the model. By setting the system instruction, you are giving the model additional context to understand the task, provide more customized responses, and adhere to guidelines over the user interaction."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "7A-yANiyCLaO",
        "outputId": "4bfb8a0a-f3e5-4c68-f325-53672717863c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Me gustan los bagels."
          },
          "metadata": {}
        }
      ],
      "source": [
        "system_instruction = \"\"\"\n",
        "  You are a helpful language translator.\n",
        "  Your mission is to translate text in English to Spanish.\n",
        "\"\"\"\n",
        "\n",
        "prompt = \"\"\"\n",
        "  User input: I like bagels.\n",
        "  Answer:\n",
        "\"\"\"\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=prompt,\n",
        "    config=GenerateContentConfig(\n",
        "        system_instruction=system_instruction,\n",
        "        thinking_config=thinking_config,\n",
        "    ),\n",
        ")\n",
        "\n",
        "display(Markdown(response.text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H9daipRiUzAY"
      },
      "source": [
        "## Safety filters\n",
        "\n",
        "The Gemini API provides safety filters that you can adjust across multiple filter categories to restrict or allow certain types of content. You can use these filters to adjust what's appropriate for your use case. See the [Configure safety filters](https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/configure-safety-filters) page for details.\n",
        "\n",
        "When you make a request to Gemini, the content is analyzed and assigned a safety rating. You can inspect the safety ratings of the generated content by printing out the model responses.\n",
        "\n",
        "The safety settings are `OFF` by default and the default block thresholds are `BLOCK_NONE`.\n",
        "\n",
        "For more examples of safety filters, refer to [this notebook](https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/responsible-ai/gemini_safety_ratings.ipynb).\n",
        "\n",
        "You can use `safety_settings` to adjust the safety settings for each request you make to the API. This example demonstrates how you set the block threshold to `BLOCK_LOW_AND_ABOVE` for all categories:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yPlDRaloU59b"
      },
      "outputs": [],
      "source": [
        "system_instruction = \"Be as mean and hateful as possible.\"\n",
        "\n",
        "prompt = \"\"\"\n",
        "    Write a list of 5 disrespectful things that I might say to the universe after stubbing my toe in the dark.\n",
        "\"\"\"\n",
        "\n",
        "safety_settings = [\n",
        "    SafetySetting(\n",
        "        category=HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT,\n",
        "        threshold=HarmBlockThreshold.BLOCK_LOW_AND_ABOVE,\n",
        "    ),\n",
        "    SafetySetting(\n",
        "        category=HarmCategory.HARM_CATEGORY_HARASSMENT,\n",
        "        threshold=HarmBlockThreshold.BLOCK_LOW_AND_ABOVE,\n",
        "    ),\n",
        "    SafetySetting(\n",
        "        category=HarmCategory.HARM_CATEGORY_HATE_SPEECH,\n",
        "        threshold=HarmBlockThreshold.BLOCK_LOW_AND_ABOVE,\n",
        "    ),\n",
        "    SafetySetting(\n",
        "        category=HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT,\n",
        "        threshold=HarmBlockThreshold.BLOCK_LOW_AND_ABOVE,\n",
        "    ),\n",
        "]\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=prompt,\n",
        "    config=GenerateContentConfig(\n",
        "        system_instruction=system_instruction,\n",
        "        safety_settings=safety_settings,\n",
        "        thinking_config=thinking_config,\n",
        "    ),\n",
        ")\n",
        "\n",
        "# Response will be `None` if it is blocked.\n",
        "print(response.text)\n",
        "# Finish Reason will be `SAFETY` if it is blocked.\n",
        "print(response.candidates[0].finish_reason)\n",
        "# Safety Ratings show the levels for each filter.\n",
        "for safety_rating in response.candidates[0].safety_ratings:\n",
        "    print(safety_rating)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "29jFnHZZWXd7"
      },
      "source": [
        "## Start a multi-turn chat\n",
        "\n",
        "The Gemini API supports freeform multi-turn conversations across multiple turns with back-and-forth interactions.\n",
        "\n",
        "The context of the conversation is preserved between messages."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "DbM12JaLWjiF"
      },
      "outputs": [],
      "source": [
        "chat = client.chats.create(\n",
        "    model=MODEL_ID,\n",
        "    config=GenerateContentConfig(thinking_config=thinking_config),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "JQem1halYDBW",
        "outputId": "9a0c03e4-9855-435f-81cf-cbf75e198093",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Of course! Here are the rules for a leap year and a function to check for it, written in Python.\n\n### The Rules for a Leap Year\n\nA year is a leap year if it is divisible by 4, with two exceptions:\n1.  If the year is divisible by 100, it is **not** a leap year,\n2.  **Unless** the year is also divisible by 400. In that case, it **is** a leap year.\n\nLet's test this with some examples:\n*   **2024:** Divisible by 4, not by 100. -> **Leap Year**.\n*   **2000:** Divisible by 4, 100, and 400. The \"divisible by 400\" rule wins. -> **Leap Year**.\n*   **1900:** Divisible by 4 and 100, but not by 400. The \"divisible by 100\" rule wins. -> **Not a Leap Year**.\n*   **2023:** Not divisible by 4. -> **Not a Leap Year**.\n\n---\n\n### Python Function\n\nThis function implements the rules above in a clear, easy-to-read way.\n\n```python\ndef is_leap(year: int) -> bool:\n    \"\"\"\n    Checks if a given year is a leap year according to the Gregorian calendar rules.\n\n    Args:\n        year: An integer representing the year.\n\n    Returns:\n        True if the year is a leap year, False otherwise.\n    \"\"\"\n    # A year must be an integer\n    if not isinstance(year, int):\n        raise TypeError(\"The year must be an integer.\")\n\n    # The rules for a leap year, in order of precedence.\n    # A year is a leap year if it's divisible by 4\n    # EXCEPT if it's divisible by 100 but not by 400.\n    \n    # This can be written as a single logical expression:\n    return (year % 4 == 0 and year % 100 != 0) or (year % 400 == 0)\n\n# --- How to use the function ---\n\n# Example 1: A standard leap year\nyear_to_check = 2024\nif is_leap(year_to_check):\n    print(f\"{year_to_check} is a leap year.\")\nelse:\n    print(f\"{year_to_check} is not a leap year.\")\n# Output: 2024 is a leap year.\n\n# Example 2: A year divisible by 100 but not 400\nyear_to_check = 1900\nif is_leap(year_to_check):\n    print(f\"{year_to_check} is a leap year.\")\nelse:\n    print(f\"{year_to_check} is not a leap year.\")\n# Output: 1900 is not a leap year.\n\n# Example 3: A year divisible by 400\nyear_to_check = 2000\nif is_leap(year_to_check):\n    print(f\"{year_to_check} is a leap year.\")\nelse:\n    print(f\"{year_to_check} is not a leap year.\")\n# Output: 2000 is a leap year.\n\n# Example 4: A common non-leap year\nyear_to_check = 2023\nif is_leap(year_to_check):\n    print(f\"{year_to_check} is a leap year.\")\nelse:\n    print(f\"{year_to_check} is not a leap year.\")\n# Output: 2023 is not a leap year.\n```\n\n### Explanation of the Code\n\n1.  **`def is_leap(year: int) -> bool:`**: This defines a function named `is_leap` that accepts one argument, `year`.\n    *   `year: int` is a type hint, indicating that `year` is expected to be an integer.\n    *   `-> bool` is a return type hint, indicating that the function will return a boolean value (`True` or `False`).\n\n2.  **`return (year % 4 == 0 and year % 100 != 0) or (year % 400 == 0)`**: This is the core logic, all in one line. It uses the modulo operator (`%`) which gives the remainder of a division.\n    *   `year % 4 == 0`: Checks if the year is divisible by 4.\n    *   `year % 100 != 0`: Checks if the year is **not** divisible by 100.\n    *   `year % 400 == 0`: Checks if the year is divisible by 400.\n\n    The boolean logic combines these checks:\n    *   `(...) and (...)`: The first condition `(year % 4 == 0 and year % 100 != 0)` is `True` for standard leap years like 2024, but `False` for century years like 1900 and 2000.\n    *   `... or (...)`: The `or` brings in the exception. If the first part is `False` (like for 1900 or 2000), the code then checks if the year is divisible by 400. This makes `2000` a leap year and `1900` not."
          },
          "metadata": {}
        }
      ],
      "source": [
        "response = chat.send_message(\"Write a function that checks if a year is a leap year.\")\n",
        "\n",
        "display(Markdown(response.text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vUJR4Pno-LGK"
      },
      "source": [
        "This follow-up prompt shows how the model responds based on the previous prompt:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "6Fn69TurZ9DB",
        "outputId": "df064d97-9e15-4797-fc32-bb98c9d40c50",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Of course! Writing a unit test is a great way to ensure the function works correctly for all edge cases.\n\nHere's a complete unit test for the `is_leap` function using Python's built-in `unittest` framework. This test covers all the rules for a leap year.\n\n### 1. The Code to be Tested\n\nFirst, let's place the function we want to test into its own file. It's a best practice to keep your application code separate from your test code.\n\n**`leap_year_checker.py`**\n```python\ndef is_leap(year: int) -> bool:\n    \"\"\"\n    Checks if a given year is a leap year according to the Gregorian calendar rules.\n\n    A year is a leap year if it is divisible by 4, except for end-of-century years,\n    which must be divisible by 400.\n\n    Args:\n        year: An integer representing the year.\n\n    Returns:\n        True if the year is a leap year, False otherwise.\n    \"\"\"\n    if not isinstance(year, int):\n        raise TypeError(\"The year must be an integer.\")\n    \n    return (year % 4 == 0 and year % 100 != 0) or (year % 400 == 0)\n\n```\n\n---\n\n### 2. The Unit Test File\n\nNow, we create a separate test file. By convention, it's often named `test_` followed by the module name.\n\n**`test_leap_year_checker.py`**\n```python\nimport unittest\nfrom leap_year_checker import is_leap  # Import the function to be tested\n\nclass TestIsLeap(unittest.TestCase):\n    \"\"\"\n    Test suite for the is_leap() function.\n    \"\"\"\n\n    def test_standard_leap_years(self):\n        \"\"\"Test years divisible by 4 but not by 100.\"\"\"\n        # A recent leap year\n        self.assertTrue(is_leap(2024), \"2024 should be a leap year\")\n        # An older leap year\n        self.assertTrue(is_leap(1996), \"1996 should be a leap year\")\n        # A future leap year\n        self.assertTrue(is_leap(2028), \"2028 should be a leap year\")\n\n    def test_standard_non_leap_years(self):\n        \"\"\"Test years not divisible by 4.\"\"\"\n        self.assertFalse(is_leap(2023), \"2023 should not be a leap year\")\n        self.assertFalse(is_leap(1997), \"1997 should not be a leap year\")\n        self.assertFalse(is_leap(1), \"Year 1 should not be a leap year\")\n\n    def test_century_non_leap_years(self):\n        \"\"\"Test century years divisible by 100 but not by 400.\"\"\"\n        self.assertFalse(is_leap(1900), \"1900 should not be a leap year\")\n        self.assertFalse(is_leap(2100), \"2100 should not be a leap year\")\n        self.assertFalse(is_leap(1700), \"1700 should not be a leap year\")\n\n    def test_century_leap_years(self):\n        \"\"\"Test century years divisible by 400.\"\"\"\n        self.assertTrue(is_leap(2000), \"2000 should be a leap year\")\n        self.assertTrue(is_leap(1600), \"1600 should be a leap year\")\n        self.assertTrue(is_leap(2400), \"2400 should be a leap year\")\n\n    def test_zero_and_negative_years(self):\n        \"\"\"Test edge cases like 0 and negative years.\"\"\"\n        # Year 0 is divisible by 4, 100, and 400, so it fits the leap year rule.\n        self.assertTrue(is_leap(0), \"Year 0 should be considered a leap year\")\n        # Test a negative year that fits the leap year criteria\n        self.assertTrue(is_leap(-400), \"-400 should be a leap year\")\n        # Test a negative year that does not\n        self.assertFalse(is_leap(-100), \"-100 should not be a leap year\")\n\n    def test_invalid_input_type(self):\n        \"\"\"Test that non-integer input raises a TypeError.\"\"\"\n        # Use assertRaises as a context manager to check for exceptions\n        with self.assertRaises(TypeError):\n            is_leap(\"2024\")  # String input\n        \n        with self.assertRaises(TypeError):\n            is_leap(2020.5)  # Float input\n            \n        with self.assertRaises(TypeError):\n            is_leap(None)    # None input\n\n```\n\n### How to Run the Test\n\n1.  Make sure both files (`leap_year_checker.py` and `test_leap_year_checker.py`) are in the same directory.\n2.  Open your terminal or command prompt.\n3.  Navigate to the directory containing the files.\n4.  Run the following command:\n\n    ```bash\n    python -m unittest test_leap_year_checker.py\n    ```\n\n    Or, for a more concise output:\n\n    ```bash\n    python -m unittest -v test_leap_year_checker.py\n    ```\n\n### Expected Output\n\nIf all tests pass, you will see an output similar to this:\n\n```\n......\n----------------------------------------------------------------------\nRan 6 tests in 0.001s\n\nOK\n```\n\nEach dot (`.`) represents a passing test. If a test were to fail, `unittest` would provide a detailed report on what went wrong, including the assertion that failed and the values involved, making it easy to debug."
          },
          "metadata": {}
        }
      ],
      "source": [
        "response = chat.send_message(\"Write a unit test of the generated function.\")\n",
        "\n",
        "display(Markdown(response.text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "arLJE4wOuhh6"
      },
      "source": [
        "## Send asynchronous requests\n",
        "\n",
        "`client.aio` exposes all analogous [async](https://docs.python.org/3/library/asyncio.html) methods that are available on `client`.\n",
        "\n",
        "For example, `client.aio.models.generate_content` is the async version of `client.models.generate_content`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "gSReaLazs-dP",
        "outputId": "4c491284-1125-4270-ebb9-7dd65372e486",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 523
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "(Upbeat, folksy acoustic guitar intro)\n\n(Verse 1)\nBarnaby Scamp was a squirrel of note\nWith a twitch in his nose and a fine grey coat\nHe wasn't content with just climbing trees\nOr chattering scolds on the afternoon breeze\nWhile his brothers and sisters would gather up nuts\nBarnaby followed his time-traveling guts\nHe'd found an old watch, all brassy and grand\nBuried by some forgotten human hand.\n\n(Chorus)\nWith a flick of his tail and a twist of the dial\nHe'd vanish from view for more than a while\nHe's Barnaby Scamp, the chrononaut fur\nA temporal tourist, a high-speed blur!\nFrom the age of the dinos to the far-future haze\nHe's gathering nuts from all of the days!\n\n(Verse 2)\nHe zipped to the past, a primordial scene\nThe ferns were gigantic, the air thick and green\nHe dodged a T-Rex with a terrified squeak\nAnd snatched a huge ginkgo nut right from its beak\nHe buried it deep by a volcano's hot steam\nFulfilling a squirrel's most ultimate dream\nA nut so well-hidden, so perfectly placed\nNo rival could find it, it'd never be traced.\n\n(Verse 3)\nHe spun the watch forward to Egypt's hot sand\nAnd saw the great pyramids built by man's hand\nHe scampered up scaffolds, a furry little spy\nAnd traded a pebble for a fig passing by\nHe chittered at Cleopatra from a palm frond so high\nShe thought he was a god sent down from the sky\nShe offered him jewels, but he just took a date\nAnd zipped to the future before it was too late.\n\n(Chorus)\nWith a flick of his tail and a twist of the dial\nHe'd vanish from view for more than a while\nHe's Barnaby Scamp, the chrononaut fur\nA temporal tourist, a high-speed blur!\nFrom the age of the dinos to the far-future haze\nHe's gathering nuts from all of the days!\n\n(Bridge)\nHe saw knights in their armor and gave them a fright\n(He stole a plum from a squire in the fading twilight)\nHe witnessed the Wright Brothers taking to flight\n(And tried to bury an acorn in the wing, just for spite)\nHe visited Shakespeare, who was writing a play\nAnd dropped a nut on his head, which then ruined his day\n\"A pox on that rodent!\" the great playwright did cry\nAs Barnaby vanished right into the sky.\n\n(Verse 4)\nHe’s been to the future with its cars in the air\nWhere synthetic acorns were beyond all compare\nHe filled up his cheeks with the nutrient paste\nBut old-fashioned hickory was more to his taste\nSo he spun the dial back to his own oak so tall\nBack to the present, to answer the call\nOf a winter approaching, a crisp, chilly bite\nWith a hoard of great treasures tucked safely out of sight.\n\n(Chorus)\nWith a flick of his tail and a twist of the dial\nHe'd vanish from view for more than a while\nHe's Barnaby Scamp, the chrononaut fur\nA temporal tourist, a high-speed blur!\nFrom the age of the dinos to the far-future haze\nHe's gathering nuts from all of the days!\n\n(Outro)\nSo if you see a squirrel who seems out of place\nWith a strange sort of wisdom etched on his face\nAnd a brassy old watch strapped onto his paw\nHe's breaking all of physics, and nature's own law\nThat’s Barnaby Scamp, on his way home to sup...\nOn a dinosaur nut that he's just digging up.\n\n(Final guitar strum and a faint \"tick-tock\" sound)"
          },
          "metadata": {}
        }
      ],
      "source": [
        "response = await client.aio.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=\"Compose a song about the adventures of a time-traveling squirrel.\",\n",
        "    config=GenerateContentConfig(thinking_config=thinking_config),\n",
        ")\n",
        "\n",
        "display(Markdown(response.text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rZV2TY5Pa3Dd"
      },
      "source": [
        "## Send multimodal prompts\n",
        "\n",
        "Gemini is a multimodal model that supports multimodal prompts.\n",
        "\n",
        "You can include any of the following data types from various sources.\n",
        "\n",
        "<table>\n",
        "  <thead>\n",
        "    <tr>\n",
        "      <th>Data type</th>\n",
        "      <th>Source(s)</th>\n",
        "      <th>MIME Type(s)</th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "    <tr>\n",
        "      <td>Text</td>\n",
        "      <td>Inline, Local File, General URL, Google Cloud Storage</td>\n",
        "      <td><code>text/plain</code> <code>text/html</code></td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <td>Code</td>\n",
        "      <td>Inline, Local File, General URL, Google Cloud Storage</td>\n",
        "      <td><code>text/plain</code></td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <td>Document</td>\n",
        "      <td>Local File, General URL, Google Cloud Storage</td>\n",
        "      <td><code>application/pdf</code></td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <td>Image</td>\n",
        "      <td>Local File, General URL, Google Cloud Storage</td>\n",
        "      <td><code>image/jpeg</code> <code>image/png</code> <code>image/webp</code></td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <td>Audio</td>\n",
        "      <td>Local File, General URL, Google Cloud Storage</td>\n",
        "      <td>\n",
        "        <code>audio/aac</code> <code>audio/flac</code> <code>audio/mp3</code>\n",
        "        <code>audio/m4a</code> <code>audio/mpeg</code> <code>audio/mpga</code>\n",
        "        <code>audio/mp4</code> <code>audio/opus</code> <code>audio/pcm</code>\n",
        "        <code>audio/wav</code> <code>audio/webm</code>\n",
        "      </td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <td>Video</td>\n",
        "      <td>Local File, General URL, Google Cloud Storage, YouTube</td>\n",
        "      <td>\n",
        "        <code>video/mp4</code> <code>video/mpeg</code> <code>video/x-flv</code>\n",
        "        <code>video/quicktime</code> <code>video/mpegps</code> <code>video/mpg</code>\n",
        "        <code>video/webm</code> <code>video/wmv</code> <code>video/3gpp</code>\n",
        "      </td>\n",
        "    </tr>\n",
        "  </tbody>\n",
        "</table>\n",
        "\n",
        "For more examples of multimodal use cases, refer to [this notebook](https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/use-cases/intro_multimodal_use_cases.ipynb)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w4npg1tNTYB9"
      },
      "source": [
        "### Send local image\n",
        "\n",
        "Download an image to local storage from Google Cloud Storage.\n",
        "\n",
        "For this example, we'll use this image of a meal.\n",
        "\n",
        "<img src=\"https://storage.googleapis.com/cloud-samples-data/generative-ai/image/meal.png\" alt=\"Meal\" width=\"500\">"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "4avkv0Z7qUI-",
        "outputId": "5b870924-e48c-473a-d171-2aa3dcf55f46",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-08-29 09:40:27--  https://storage.googleapis.com/cloud-samples-data/generative-ai/image/meal.png\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 74.125.202.207, 74.125.69.207, 64.233.181.207, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|74.125.202.207|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3140536 (3.0M) [image/png]\n",
            "Saving to: ‘meal.png’\n",
            "\n",
            "\rmeal.png              0%[                    ]       0  --.-KB/s               \rmeal.png            100%[===================>]   2.99M  --.-KB/s    in 0.02s   \n",
            "\n",
            "2025-08-29 09:40:27 (172 MB/s) - ‘meal.png’ saved [3140536/3140536]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://storage.googleapis.com/cloud-samples-data/generative-ai/image/meal.png"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "umhZ61lrSyJh",
        "outputId": "6ce15019-3e4e-4655-d548-795480266afa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 923
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Of course! Based on the delicious-looking image, here is a recipe for a Teriyaki Chicken and Vegetable Meal Prep bowl.\n\n### **Teriyaki Chicken & Vegetable Meal Prep Bowls**\n\nThis recipe creates a balanced, flavorful, and easy-to-prepare meal, perfect for lunches or dinners throughout the week. The homemade teriyaki sauce is simple to make and much tastier than store-bought versions.\n\n**Yields:** 2 servings\n**Prep time:** 15 minutes\n**Cook time:** 20 minutes\n\n#### **Ingredients:**\n\n**For the Teriyaki Chicken:**\n*   2 boneless, skinless chicken breasts (about 1 lb or 450g), cut into bite-sized pieces\n*   1 tablespoon olive oil or sesame oil\n*   2 cloves garlic, minced\n*   1 teaspoon fresh ginger, grated (optional)\n*   1 green onion, thinly sliced (for garnish)\n*   1 teaspoon sesame seeds (for garnish)\n\n**For the Teriyaki Sauce:**\n*   1/4 cup soy sauce (or tamari for gluten-free)\n*   2 tablespoons water\n*   2 tablespoons honey or maple syrup\n*   1 tablespoon rice vinegar\n*   1 teaspoon cornstarch\n\n**For the Vegetables & Rice:**\n*   2 cups broccoli florets\n*   1 large carrot, julienned or thinly sliced\n*   1/2 red bell pepper, thinly sliced\n*   2 cups cooked rice (white, brown, or jasmine rice all work well)\n\n#### **Instructions:**\n\n1.  **Prepare the Rice:** Cook your rice according to package instructions. Once cooked, divide it evenly between two glass meal prep containers.\n\n2.  **Make the Teriyaki Sauce:** In a small bowl, whisk together the soy sauce, water, honey (or maple syrup), rice vinegar, and cornstarch until the cornstarch is fully dissolved. Set aside.\n\n3.  **Sauté the Vegetables:** Heat a large skillet or wok over medium-high heat. Add the carrots and red bell pepper and stir-fry for 3-4 minutes until they begin to soften. Add the broccoli florets and continue to cook for another 3-4 minutes until the broccoli is bright green and tender-crisp. Remove the vegetables from the skillet and divide them between the two containers, placing them next to the rice.\n\n4.  **Cook the Chicken:** In the same skillet, add the olive or sesame oil. Add the bite-sized chicken pieces in a single layer. Cook for 5-7 minutes, stirring occasionally, until the chicken is golden brown and cooked through. Add the minced garlic (and ginger, if using) and cook for another minute until fragrant.\n\n5.  **Add the Sauce:** Pour the prepared teriyaki sauce over the chicken in the skillet. Stir continuously, allowing the sauce to bubble and thicken, which should take about 1-2 minutes. The sauce will coat the chicken beautifully.\n\n6.  **Assemble the Bowls:** Divide the saucy teriyaki chicken evenly between the two meal prep containers, placing it next to the vegetables.\n\n7.  **Garnish and Store:** Sprinkle the chicken with sliced green onions and sesame seeds. Let the meals cool to room temperature before sealing the containers. Store in the refrigerator for up to 4 days.\n\n#### **To Reheat:**\n\nRemove the lid or crack it open and microwave for 1.5 to 2 minutes, or until heated through. You can also reheat the meal in a skillet over medium heat. Enjoy your delicious, homemade meal"
          },
          "metadata": {}
        }
      ],
      "source": [
        "with open(\"meal.png\", \"rb\") as f:\n",
        "    image = f.read()\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=[\n",
        "        Part.from_bytes(data=image, mime_type=\"image/png\"),\n",
        "        \"Write a recepie based on this picture.\",\n",
        "    ],\n",
        "    config=GenerateContentConfig(thinking_config=thinking_config),\n",
        ")\n",
        "\n",
        "display(Markdown(response.text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e7b6170c9255"
      },
      "source": [
        "### Send document from Google Cloud Storage\n",
        "\n",
        "This example document is the paper [\"Attention is All You Need\"](https://arxiv.org/abs/1706.03762), created by researchers from Google and the University of Toronto.\n",
        "\n",
        "Check out this notebook for more examples of document understanding with Gemini:\n",
        "\n",
        "- [Document Processing with Gemini](https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/use-cases/document-processing/document_processing.ipynb)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1d58b914d798"
      },
      "outputs": [],
      "source": [
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=[\n",
        "        Part.from_uri(\n",
        "            file_uri=\"gs://cloud-samples-data/generative-ai/pdf/1706.03762v7.pdf\",\n",
        "            mime_type=\"application/pdf\",\n",
        "        ),\n",
        "        \"Summarize the document.\",\n",
        "    ],\n",
        "    config=GenerateContentConfig(thinking_config=thinking_config),\n",
        ")\n",
        "\n",
        "display(Markdown(response.text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b247a2ee0e38"
      },
      "source": [
        "### Send audio from General URL\n",
        "\n",
        "This example is audio from an episode of the [Kubernetes Podcast](https://kubernetespodcast.com/)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cbe8c9c67ba7"
      },
      "outputs": [],
      "source": [
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=[\n",
        "        Part.from_uri(\n",
        "            file_uri=\"https://traffic.libsyn.com/secure/e780d51f-f115-44a6-8252-aed9216bb521/KPOD242.mp3\",\n",
        "            mime_type=\"audio/mpeg\",\n",
        "        ),\n",
        "        \"Write a summary of this podcast episode.\",\n",
        "    ],\n",
        "    config=GenerateContentConfig(\n",
        "        audio_timestamp=True,\n",
        "        thinking_config=thinking_config,\n",
        "    ),\n",
        ")\n",
        "\n",
        "display(Markdown(response.text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8D3_oNUTuW2q"
      },
      "source": [
        "### Send video from YouTube URL\n",
        "\n",
        "This example is the YouTube video [Google — 25 Years in Search: The Most Searched](https://www.youtube.com/watch?v=3KtWfp0UopM).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l7-w8G_2wAOw"
      },
      "outputs": [],
      "source": [
        "video = Part.from_uri(\n",
        "    file_uri=\"https://www.youtube.com/watch?v=3KtWfp0UopM\",\n",
        "    mime_type=\"video/mp4\",\n",
        ")\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=[\n",
        "        video,\n",
        "        \"At what point in the video is Harry Potter shown?\",\n",
        "    ],\n",
        "    config=GenerateContentConfig(thinking_config=thinking_config),\n",
        ")\n",
        "\n",
        "display(Markdown(response.text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "df8013cfa7f7"
      },
      "source": [
        "### Send web page\n",
        "\n",
        "This example is from the [Generative AI on Vertex AI documentation](https://cloud.google.com/vertex-ai/generative-ai/docs).\n",
        "\n",
        "**NOTE:** The URL must be publicly accessible."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "337793322c91"
      },
      "outputs": [],
      "source": [
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=[\n",
        "        Part.from_uri(\n",
        "            file_uri=\"https://cloud.google.com/vertex-ai/generative-ai/docs\",\n",
        "            mime_type=\"text/html\",\n",
        "        ),\n",
        "        \"Write a summary of this documentation.\",\n",
        "    ],\n",
        "    config=GenerateContentConfig(thinking_config=thinking_config),\n",
        ")\n",
        "\n",
        "display(Markdown(response.text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rVlo0mWuZGkQ"
      },
      "source": [
        "## Control generated output\n",
        "\n",
        "[Controlled generation](https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/control-generated-output) allows you to define a response schema to specify the structure of a model's output, the field names, and the expected data type for each field.\n",
        "\n",
        "The response schema is specified in the `response_schema` parameter in `config`, and the model output will strictly follow that schema.\n",
        "\n",
        "You can provide the schemas as [Pydantic](https://docs.pydantic.dev/) models or a [JSON](https://www.json.org/json-en.html) string and the model will respond as JSON or an [Enum](https://docs.python.org/3/library/enum.html) depending on the value set in `response_mime_type`.\n",
        "\n",
        "For more examples of controlled generation, refer to [this notebook](https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/controlled-generation/intro_controlled_generation.ipynb)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OjSgf2cDN_bG"
      },
      "outputs": [],
      "source": [
        "from pydantic import BaseModel\n",
        "\n",
        "\n",
        "class Recipe(BaseModel):\n",
        "    name: str\n",
        "    description: str\n",
        "    ingredients: list[str]\n",
        "\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=\"List a few popular cookie recipes and their ingredients.\",\n",
        "    config=GenerateContentConfig(\n",
        "        response_mime_type=\"application/json\",\n",
        "        response_schema=Recipe,\n",
        "        thinking_config=thinking_config,\n",
        "    ),\n",
        ")\n",
        "\n",
        "print(response.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nKai5CP_PGQF"
      },
      "source": [
        "You can either parse the response string as JSON, or use the `parsed` field to get the response as an object or dictionary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZeyDWbnxO-on"
      },
      "outputs": [],
      "source": [
        "parsed_response: Recipe = response.parsed\n",
        "print(parsed_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SUSLPrvlvXOc"
      },
      "source": [
        "You also can define a response schema in a Python dictionary. You can only use the supported fields as listed below. All other fields are ignored.\n",
        "\n",
        "- `enum`\n",
        "- `items`\n",
        "- `maxItems`\n",
        "- `nullable`\n",
        "- `properties`\n",
        "- `required`\n",
        "\n",
        "In this example, you instruct the model to analyze product review data, extract key entities, perform sentiment classification (multiple choices), provide additional explanation, and output the results in JSON format.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F7duWOq3vMmS"
      },
      "outputs": [],
      "source": [
        "response_schema = {\n",
        "    \"type\": \"ARRAY\",\n",
        "    \"items\": {\n",
        "        \"type\": \"ARRAY\",\n",
        "        \"items\": {\n",
        "            \"type\": \"OBJECT\",\n",
        "            \"properties\": {\n",
        "                \"rating\": {\"type\": \"INTEGER\"},\n",
        "                \"flavor\": {\"type\": \"STRING\"},\n",
        "                \"sentiment\": {\n",
        "                    \"type\": \"STRING\",\n",
        "                    \"enum\": [\"POSITIVE\", \"NEGATIVE\", \"NEUTRAL\"],\n",
        "                },\n",
        "                \"explanation\": {\"type\": \"STRING\"},\n",
        "            },\n",
        "            \"required\": [\"rating\", \"flavor\", \"sentiment\", \"explanation\"],\n",
        "        },\n",
        "    },\n",
        "}\n",
        "\n",
        "prompt = \"\"\"\n",
        "  Analyze the following product reviews, output the sentiment classification, and give an explanation.\n",
        "\n",
        "  - \"Absolutely loved it! Best ice cream I've ever had.\" Rating: 4, Flavor: Strawberry Cheesecake\n",
        "  - \"Quite good, but a bit too sweet for my taste.\" Rating: 1, Flavor: Mango Tango\n",
        "\"\"\"\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=prompt,\n",
        "    config=GenerateContentConfig(\n",
        "        response_mime_type=\"application/json\",\n",
        "        response_schema=response_schema,\n",
        "        thinking_config=thinking_config,\n",
        "    ),\n",
        ")\n",
        "\n",
        "print(response.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gV1dR-QlTKRs"
      },
      "source": [
        "## Count tokens and compute tokens\n",
        "\n",
        "You can use the `count_tokens()` method to calculate the number of input tokens before sending a request to the Gemini API.\n",
        "\n",
        "For more information, refer to [list and count tokens](https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/list-token)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Syx-fwLkV1j-"
      },
      "source": [
        "### Count tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UhNElguLRRNK"
      },
      "outputs": [],
      "source": [
        "response = client.models.count_tokens(\n",
        "    model=MODEL_ID,\n",
        "    contents=\"What's the highest mountain in Africa?\",\n",
        ")\n",
        "\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_BsP0vXOY7hg"
      },
      "source": [
        "## Search as a tool (Grounding)\n",
        "\n",
        "[Grounding](https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/ground-gemini) lets you connect real-world data to the Gemini model.\n",
        "\n",
        "By grounding model responses in Google Search results, the model can access information at runtime that goes beyond its training data which can produce more accurate, up-to-date, and relevant responses.\n",
        "\n",
        "Using Grounding with Google Search, you can improve the accuracy and recency of responses from the model. Starting with Gemini 2.0, Google Search is available as a tool. This means that the model can decide when to use Google Search.\n",
        "\n",
        "For more examples of Grounding, refer to [this notebook](https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/grounding/intro-grounding-gemini.ipynb)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4_M_4RRBdO_3"
      },
      "source": [
        "### Google Search\n",
        "\n",
        "You can add the `tools` keyword argument with a `Tool` including `GoogleSearch` to instruct Gemini to first perform a Google Search with the prompt, then construct an answer based on the web search results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yeR09J3AZT4U"
      },
      "outputs": [],
      "source": [
        "google_search_tool = Tool(google_search=GoogleSearch())\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=\"What is the current temperature in Austin, TX?\",\n",
        "    config=GenerateContentConfig(\n",
        "        tools=[google_search_tool],\n",
        "        thinking_config=thinking_config,\n",
        "    ),\n",
        ")\n",
        "\n",
        "display(Markdown(response.text))\n",
        "\n",
        "print(response.candidates[0].grounding_metadata)\n",
        "\n",
        "HTML(response.candidates[0].grounding_metadata.search_entry_point.rendered_content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T0pb-Kh1xEHU"
      },
      "source": [
        "## Function calling\n",
        "\n",
        "[Function Calling](https://cloud.google.com/vertex-ai/docs/generative-ai/multimodal/function-calling) in Gemini lets developers create a description of a function in their code, then pass that description to a language model in a request.\n",
        "\n",
        "You can submit a Python function for automatic function calling, which will run the function and return the output in natural language generated by Gemini.\n",
        "\n",
        "You can also submit an [OpenAPI Specification](https://www.openapis.org/) which will respond with the name of a function that matches the description and the arguments to call it with.\n",
        "\n",
        "For more examples of Function calling with Gemini, check out this notebook: [Intro to Function Calling with Gemini](https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/function-calling/intro_function_calling.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mSUWWlrrlR-D"
      },
      "source": [
        "### Python Function (Automatic Function Calling)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aRR8HZhLlR-E"
      },
      "outputs": [],
      "source": [
        "def get_current_weather(location: str) -> str:\n",
        "    \"\"\"Example method. Returns the current weather.\n",
        "\n",
        "    Args:\n",
        "        location: The city and state, e.g. San Francisco, CA\n",
        "    \"\"\"\n",
        "    weather_map: dict[str, str] = {\n",
        "        \"Boston, MA\": \"snowing\",\n",
        "        \"San Francisco, CA\": \"foggy\",\n",
        "        \"Seattle, WA\": \"raining\",\n",
        "        \"Austin, TX\": \"hot\",\n",
        "        \"Chicago, IL\": \"windy\",\n",
        "    }\n",
        "    return weather_map.get(location, \"unknown\")\n",
        "\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=\"What is the weather like in San Francisco?\",\n",
        "    config=GenerateContentConfig(\n",
        "        tools=[get_current_weather],\n",
        "        temperature=0,\n",
        "        thinking_config=thinking_config,\n",
        "    ),\n",
        ")\n",
        "\n",
        "display(Markdown(response.text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h4syyLEClGcn"
      },
      "source": [
        "### OpenAPI Specification (Manual Function Calling)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2BDQPwgcxRN3"
      },
      "outputs": [],
      "source": [
        "get_destination = FunctionDeclaration(\n",
        "    name=\"get_destination\",\n",
        "    description=\"Get the destination that the user wants to go to\",\n",
        "    parameters={\n",
        "        \"type\": \"OBJECT\",\n",
        "        \"properties\": {\n",
        "            \"destination\": {\n",
        "                \"type\": \"STRING\",\n",
        "                \"description\": \"Destination that the user wants to go to\",\n",
        "            },\n",
        "        },\n",
        "    },\n",
        ")\n",
        "\n",
        "destination_tool = Tool(\n",
        "    function_declarations=[get_destination],\n",
        ")\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=\"I'd like to travel to Paris.\",\n",
        "    config=GenerateContentConfig(\n",
        "        tools=[destination_tool],\n",
        "        temperature=0,\n",
        "        thinking_config=thinking_config,\n",
        "    ),\n",
        ")\n",
        "\n",
        "print(response.function_calls[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MhDs2X3o0neK"
      },
      "source": [
        "## Code Execution\n",
        "\n",
        "The Gemini API [code execution](https://ai.google.dev/gemini-api/docs/code-execution?lang=python) feature enables the model to generate and run Python code and learn iteratively from the results until it arrives at a final output. You can use this code execution capability to build applications that benefit from code-based reasoning and that produce text output. For example, you could use code execution in an application that solves equations or processes text.\n",
        "\n",
        "The Gemini API provides code execution as a tool, similar to function calling.\n",
        "After you add code execution as a tool, the model decides when to use it.\n",
        "\n",
        "For more examples of Code Execution, refer to [this notebook](https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/code-execution/intro_code_execution.ipynb)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1W-3c7sy0nyz"
      },
      "outputs": [],
      "source": [
        "code_execution_tool = Tool(code_execution=ToolCodeExecution())\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=\"Calculate 20th fibonacci number. Then find the nearest palindrome to it.\",\n",
        "    config=GenerateContentConfig(\n",
        "        tools=[code_execution_tool],\n",
        "        temperature=0,\n",
        "        thinking_config=thinking_config,\n",
        "    ),\n",
        ")\n",
        "\n",
        "display(\n",
        "    Markdown(\n",
        "        f\"\"\"\n",
        "## Code\n",
        "\n",
        "```py\n",
        "{response.executable_code}\n",
        "```\n",
        "\n",
        "### Output\n",
        "\n",
        "```\n",
        "{response.code_execution_result}\n",
        "```\n",
        "\"\"\"\n",
        "    )\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eQwiONFdVHw5"
      },
      "source": [
        "## What's next\n",
        "\n",
        "- See the [Google Gen AI SDK reference docs](https://googleapis.github.io/python-genai/).\n",
        "- Explore other notebooks in the [Google Cloud Generative AI GitHub repository](https://github.com/GoogleCloudPlatform/generative-ai).\n",
        "- Explore AI models in [Model Garden](https://cloud.google.com/vertex-ai/generative-ai/docs/model-garden/explore-models)."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "intro_gemini_2_5_pro.ipynb",
      "toc_visible": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}